# 计算机组成原理

**1.What is “Information”？**

Information，n. Date communicated or received that resolves uncertainty a particular fact or circumstance. 

**2.Quantifying Information**
$$
I(x_i)=\log_2(1/p_i)
$$
p_i is the proportional to the uncertainty of choice x_i

**3.Entropy(熵)**

In the information theory, the entropy H(X) is the average amount of information contained in each piece of data received about the value of X;
$$
H(X)=E(I(X))=\sum_{i=1}^{N}p_i·\log_2(1/p_i)
$$
**4.Encoding**

**5.Signed Integers: 2's complement** 

**6.Varibale-length Encoding** 

**7.Huffman's Algorithm**

Given a set of symbols and their probabilities, construct an optimal variable-length encoding. 

**Huffman's Algorithm:**

- Build subtree using 2 symbols with lowest p_i
- At each step choose two symbols/subtrees with lowest p_i, combine to form new subtree
- Result: optimal tree built from the bottom-up.

**8.Error Detection and Correction** 

**9.Error Correction** 